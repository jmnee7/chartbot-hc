#!/usr/bin/env python3
"""
ë²…ìŠ¤ í¬ë¡¤ëŸ¬ ë””ë²„ê¹…ìš© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import sys
import os

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from crawlers.bugs_crawler import BugsCrawler
from target_songs import TARGET_SONG

def test_bugs_crawler():
    print("=== ë²…ìŠ¤ í¬ë¡¤ëŸ¬ ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ ===")
    
    crawler = BugsCrawler()
    url = crawler.get_chart_url()
    print(f"í¬ë¡¤ë§ URL: {url}")
    
    # ì‹¤ì œ ìš”ì²­ í…ŒìŠ¤íŠ¸
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print("\n1. HTTP ìš”ì²­ í…ŒìŠ¤íŠ¸...")
        response = requests.get(url, headers=headers)
        print(f"ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        if response.status_code != 200:
            print("âŒ HTTP ìš”ì²­ ì‹¤íŒ¨")
            return
            
        print("âœ… HTTP ìš”ì²­ ì„±ê³µ")
        
        print("\n2. HTML íŒŒì‹± í…ŒìŠ¤íŠ¸...")
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # í˜„ì¬ í¬ë¡¤ëŸ¬ì˜ ì…€ë ‰í„°ë¡œ ìš”ì†Œ ì°¾ê¸°
        elements = crawler.get_song_elements(soup)
        print(f"ë°œê²¬ëœ ìš”ì†Œ ìˆ˜: {len(elements)}")
        
        if not elements:
            print("âŒ ì…€ë ‰í„°ë¡œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print("\ní˜ì´ì§€ êµ¬ì¡° ë¶„ì„...")
            
            # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
            selectors_to_try = [
                ".list.trackList.chart .track-list li",
                ".trackList li",
                ".chart li",
                ".list li",
                "li",
                "tbody tr"
            ]
            
            for sel in selectors_to_try:
                found = soup.select(sel)
                print(f"  {sel}: {len(found)}ê°œ")
                
            # í˜ì´ì§€ ì œëª© í™•ì¸
            title = soup.find('title')
            if title:
                print(f"\ní˜ì´ì§€ ì œëª©: {title.text}")
                
            return
        
        print("âœ… ìš”ì†Œ ë°œê²¬ë¨")
        
        print("\n3. ë°ì´í„° íŒŒì‹± í…ŒìŠ¤íŠ¸...")
        target_song = TARGET_SONG  # íƒ€ê²Ÿ ê³¡ ì‚¬ìš©
        print(f"íƒ€ê²Ÿ ê³¡: {target_song['title']} - {target_song['artist']}")
        
        found_ranks = []
        for i, element in enumerate(elements[:10]):  # ìƒìœ„ 10ê°œë§Œ í™•ì¸
            try:
                data = crawler.parse_song_data(element)
                if data:
                    print(f"  {i+1}: {data.get('rank', '?')}ìœ„ - {data.get('title', '?')} - {data.get('artist', '?')}")
                    
                    # íƒ€ê²Ÿ ê³¡ ë§¤ì¹­ í™•ì¸
                    if crawler.is_target_song(data, target_song):
                        found_ranks.append(data['rank'])
                        print(f"  ğŸ¯ íƒ€ê²Ÿ ê³¡ ë°œê²¬! {data['rank']}ìœ„")
                        
            except Exception as e:
                print(f"  {i+1}: íŒŒì‹± ì˜¤ë¥˜ - {e}")
        
        if found_ranks:
            print(f"\nâœ… íƒ€ê²Ÿ ê³¡ ìˆœìœ„: {found_ranks}")
        else:
            print(f"\nâŒ íƒ€ê²Ÿ ê³¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_bugs_crawler()
